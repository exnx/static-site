<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Eric Nguyen - Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="static/css/main.css" />
		<noscript><link rel="stylesheet" href="static/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">Welcome</a></li>
							<li><a href="#one">Research</a></li>
							<li><a href="#two">Projects</a></li>
							<li><a href="#three">Education</a></li>
							<li><a href="#four">Work Experience</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
					<section id="intro" class="wrapper style1 fullscreen fade-up">
						<div class="inner">
							<h1>I'm Eric</h1>
							<p>I work on AI with applications in vision, language and biology</p>
                            <div>
                                <div>
                                   <img src="static/images/head_square_main.png" width="191" height="200" alt="hyena_dna_paper" align="middle"/>
                                </div>                                
                                <br>
                                <div>
                                    <ul class="actions">
                                        <li><a href="https://www.linkedin.com/in/nguyenstanford/" class="button scrolly">LinkedIn</a></li>
                                    </ul>

                                    <ul class="actions">
                                        <li><a href="https://github.com/HazyResearch" class="button">HazyResearch Github</a></li>
                                    </ul>                            

                                    <ul class="actions">
                                        <li><a href="https://github.com/exnx" class="button">Personal Github</a></li>
                                    </ul>

                                    <ul class="actions">
                                        <li><a href="Eric-Nguyen-Resume.pdf" download="" class="button">Download Resume</a></li>
                                    </ul>
                                </div>

                            </div>
                                
                            <h2>About</h2>
                        
                            <p>I'm a PhD student at Stanford in the BioEngineering department. I'm advised by <a href="https://baccuslab.github.io/">Steve Baccus</a> in neurobiology and <a href="https://cs.stanford.edu/~chrismre/">Chris RÃ©</a> in computer science. I'm a part of the <a href="https://baccuslab.github.io/">Baccus lab</a> and <a href="https://hazyresearch.stanford.edu/">Hazy Research</a>.
							 </p>       
                            
                            <h2>Updates</h2>

                            <p>04/04/25:  I'll be giving a <a href="https://conferences.ted.com/ted2025/speakers"><b>Ted2025</b></a> talk next week in Vancouver, on the future of AI and biology. I'm incredibly excited!  </p>                               
                            
                            <p>11/15/24:  Evo has been published on the cover of <a href="https://www.science.org/doi/10.1126/science.ado9336"><b>Science</b></a>! What an accomplishment by the team! </p>            
                            
                            <p>05/07/24:  Our <a href="https://arxiv.org/abs/2403.17844"><b>paper</b></a> on Mechanistic Architecture Design (MAD) has been published at ICLR 2024! I love this team :) It describes how we designed and scaled the architecture for Evo!  What an accomplishment by the team! </p>                               
                            <p>02/27/24:  Our preprint on <a href="https://arcinstitute.org/manuscripts/Evo"><b>Evo</b></a> is announced! We try to answer if "DNA is all you need" for a biological foundation model.</p>                              
                            
                            <p>01/15/24:  Our ICLR '24 paper on <a href="https://arxiv.org/abs/2311.05908"><b>FlashFFTConv</b></a> has been accepted! Excited to visit Vienna, Austria, never been!</p>                   
                                                          
                            <p>06/28/23:  I'm very excited to share <a href="#hyena_dna"><b>HyenaDNA</b></a>, a long-range foundation model for DNA! (update, accepted at NeurIPS '23 as a *spotlight* !)
                                <a href="https://arxiv.org/abs/2306.15794"><b>arxiv</b></a>,
                                <a href="https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna"><b>blog</b></a>,
                                <a href="https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL"><b>colab</b></a>,
                                <a href="https://github.com/HazyResearch/safari"><b>github</b></a>,
                                <a href="https://huggingface.co/LongSafari"><b>checkpoints</b></a>,
                                <a href="https://www.youtube.com/watch?v=haSkAC1fPX0"><b>YouTube talk</b></a>
                            </p>
                            
                            <p>04/24/23:  Our ICML '23 paper on <a href="https://arxiv.org/abs/2302.10866"><b>Hyena</b></a> has been accepted! And as an oral presentation, that's a first! Very fortunate to be a part of it.</p>                            

                            <p>03/07/23:  Excited to share our work on Hyena, an alternative to attention that can learn on sequences *10x longer*, is up to *100x faster* than optimized attention, by using implicit long convolutions & gating! <a href="https://arxiv.org/abs/2302.10866"><b>arxiv</b></a>, <a href="https://github.com/HazyResearch/hyena-dna"><b>code</b></a>, <a href="https://hazyresearch.stanford.edu/blog/2023-03-07-hyena"><b>blog</b></a></p>
                            
                            <p>09/14/22:  Our NeurIPS '22 paper on <a href="https://arxiv.org/abs/2210.06583"><b>S4ND</b></a>  has been accepted!!! I am SO excited! We extend work on S4 to muldimensional continuous-signals like images and video.</p>
                            
                            <p>06/21/22:  I started my 2nd internship with Google Research on the Machine Intelligence and Image Understanding team, working on text-guided image generation!</p>
                            
                            <p>05/19/22:  We submitted our paper on S4ND to NeurIPS 2022, an extension of S4 to multidimensional signals for modeling images and video!  Fingers crossed...!</p>                             
                            
                            <p>12/01/21:  I officially joined Steve Baccus' lab in neurobiology for my thesis lab! Steve studies the visual system in humans and animals.  I'll also be co-advised by Chris Re in computer science! I'm excited to fuse neuroscience and AI!</p> 
                            
                            <p>07/22/21:  My paper, <a href="#adobe"><b>OSCAR-Net</b></a>, just got accepted into ICCV 2021! So excited for my first paper...!</p>
                            
                            <p>06/15/21:  I started my internship at Google Research! I'll be working on multimodal generation (for joint video and audio)!</p>  
                            
                            <p>01/05/21:  I started a (joint) lab rotation with Chris Re and Fei-Fei Li, continuing work on CPR quality prediction in videos.</p>
                            
                            <p><a href="https://aicare.stanford.edu/">12/1/20:  I'll be joining Fei Fei Li's lab in January for a rotation! I'll be in the Partnership in AI-Assisted Care - think smart hospitals with computer vision.</a></p>
                            
                            <p><a href="#adobe">11/16/20:  I submitted my first paper to CVPR 2021 on my work at Adobe, and we're patenting the algorithm!  (Update - got my first conference rejection!)</a></p>
                            
                            <p>09/21/20:  I started my first lab rotation with Leo Guibas in computer science.  I'll be working on detecting walkable floor space for an assistive-robotic suit.</p>
                           
<!--
                            <p><a href="https://youtu.be/7SZm1uTMypc">
                               09/11/20:  my first YouTube video on "Starting a PhD at Stanford during COVID"</a></p>
                            
                            <div class="flex-container">
                                <div> <iframe width="400" height="250" src="https://www.youtube.com/embed/7SZm1uTMypc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                            </div>
-->
                            
                            
<!--
                           <p><a href="https://medium.com/@ericnguyen_54929/moving-from-manhattan-to-the-bay-area-during-peak-covid-cca3829bd1fd">
                               05/22/20:  wrote my first Medium article, "Moving from Manhattan to the Bay Area during peak COVID"</a></p>
-->
                
						  </div>  
                        
					</section>

					<section id="one" class="wrapper style2 spotlights">
						<section id="evo">
                            <div class="content_left">
                                <img src="static/images/ball-of-life-zoom.gif" width="410" height="370" alt="ball-of-life" align="middle"/>
                            </div>                          
							<div class="content">
								<div class="inner">

                                    <a href="https://www.science.org/doi/10.1126/science.ado9336"><h2>Sequence modeling and design from molecular to genome scale with Evo</h2></a>
                                
                                    <p>We report Evo, a genomic foundation model that enables prediction and generation tasks from the molecular to genome scale. Using an architecture based on advances in deep signal processing, we scale Evo to 7 billion parameters with a context length of 131 kilobases (kb) at single-nucleotide, byte resolution. Trained on whole prokaryotic genomes, Evo can generalize across the three fundamental modalities of the central dogma of molecular biology to perform zero-shot function prediction that is competitive with, or outperforms, leading domain-specific language models. Evo also excels at multi-element generation tasks, which we demonstrate by generating synthetic CRISPR-Cas molecular complexes and entire transposable systems for the first time. Using information learned over whole genomes, Evo can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to 650 kb in length, orders of magnitude longer than previous methods. Advances in multi-modal and multi-scale learning with Evo provides a promising path toward improving our understanding and control of biology across multiple levels of complexity.
                                    </p>
									<ul class="actions">
										<li><a href="https://www.youtube.com/live/rKy6O9iQvGU?si=5DZMegIcg0v2jP04" class="button">YouTube talk</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://arcinstitute.org/news/blog/evo" class="button">blog</a></li>
									</ul>    
									<ul class="actions">
										<li><a href="https://arcinstitute.org/manuscripts/Evo" class="button">paper</a></li>
									</ul>
                                      
									<ul class="actions">
										<li><a href="https://github.com/evo-design/evo" class="button">github</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://huggingface.co/togethercomputer/evo-1-131k-base" class="button">checkpoints</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://pypi.org/project/evo-model/0.1.0/" class="button">pip install</a></li>
									</ul>                                      
								</div>
							</div>
						</section>       
						<section id="hyena_dna">
                            <div class="content_left">
                               <a href="https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna"><img src="static/images/cute-hyena-dna.jpg" width="400" height="400" alt="hyena_dna_paper" align="middle"/></a>
                               <a href="https://www.youtube.com/watch?v=haSkAC1fPX0"><img src="static/images/hyenadna_talk.png" width="500" height="250" alt="hyena_dna_youtube" align="middle"/></a>
                            </div>
							<div class="content">
								<div class="inner">

                                    <a href="https://arxiv.org/abs/2306.15794"><h2>HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution (NeurIPS '23, Spotlight)</h2></a>
                                
                                    <p> HyenaDNA is a long-range genomic foundation model pretrained on the human genome with context lengths of up to <strong>1 million tokens</strong> at single nucleotide resolution. It uses a simple stack of Hyena operators - a new layer based on implicit convolutions that's been shown to match attention in quality in natural language with lower time complexity. HyenaDNA enables sequences <strong>500x longer</strong> and trains <strong>160x faster</strong> than previous Transformer-based genomic models. HyenaDNA achieves state-of-the-art on 23 (of 28) genomic downstream tasks, and explores the first use of in-context learning in genomics. Our entire <strong><a href="https://github.com/HazyResearch/hyena-dna">codebase</a></strong> is public! Go biology :)
                                    </p>
									<ul class="actions">
										<li><a href="https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna" class="button">blog</a></li>
									</ul>    
									<ul class="actions">
										<li><a href="https://www.youtube.com/watch?v=haSkAC1fPX0" class="button">YouTube talk</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://arxiv.org/abs/2306.15794" class="button">arxiv</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL" class="button">colab</a></li>
									</ul>                                    
									<ul class="actions">
										<li><a href="https://github.com/HazyResearch/hyena-dna" class="button">github</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://huggingface.co/LongSafari" class="button">checkpoints</a></li>
									</ul>                                    
								</div>
							</div>
						</section>                          
						<section>
                            <div class="content_left">
                               <a href="https://arxiv.org/abs/2302.10866"><img src="static/images/hyena.png" width="500" height="375" alt="hyena_paper" align="middle"/></a>
                            </div>
							<div class="content">
								<div class="inner">

                                    <a href="https://arxiv.org/abs/2302.10866"><h2>Hyena Hierarchy: Towards Larger Convolutional Language Models (ICML '23, Oral Presentation)</h2></a>
                                
									<p> Hyena is an attention-replacement operator that can in-context learn on sequences 10x longer, is up to 100x faster than optimized attention, has lower time complexity, by using long convolutions & gating. Hyena is a convolutional operator for large language models that can match attention quality, while scaling subquadratically in sequence length, allowing us to train 100x faster at 64k tokens, and train on sequences up to 131k tokens long. Just like attention, Hyena can be used in Vision Transformer - matching attention on ImageNet-1k, demonstrating potential for Hyena as a general deep learning operator.
                                    </p>
                                    
									<ul class="actions">
										<li><a href="https://arxiv.org/abs/2302.10866" class="button">arxiv</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://github.com/HazyResearch/safari" class="button">code</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://hazyresearch.stanford.edu/blog/2023-03-07-hyena" class="button">blog</a></li>
									</ul>
								</div>
							</div>
						</section>
                        
						<section>
                            <div class="content_left">
                               <a href="https://www.youtube.com/watch?v=ubXV5PEKPNE"><img src="static/images/slideslive.jpg" width="500" height="280" alt="S4ND_paper" align="middle"/></a>
                            </div>
							<div class="content">
								<div class="inner">

                                    <a href="https://arxiv.org/abs/2210.06583"><h2>S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces (NeurIPS 2022)</h2></a>
                                
									<p> The visual world is made up of multidimensional and naturally continuous-signals, but SotA computer vision models, e.g., Transformers and CNNs, use discrete pixel representations. We present S4ND, a new deep learning layer for computer vision models, that learns continuous-signal representations of images and videos by extending work on S4 <a href="https://arxiv.org/abs/2111.00396">(Gu. et al)</a> into multiple dimensions. We show S4ND can boost or maintain performance of canonical vision architectures by replacing standard 2D/3D convolutions and self-attention with a continuous and global convolutional kernel.
                                    </p>
                                    
									<ul class="actions">
										<li><a href="https://arxiv.org/abs/2210.06583" class="button">arxiv</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://github.com/HazyResearch/state-spaces/tree/main/configs/experiment/s4nd" class="button">Code</a></li>
									</ul>
									<ul class="actions">
										<li><a href="https://hazyresearch.stanford.edu/blog" class="button">Blog coming soon!</a></li>
									</ul>
								</div>
							</div>
						</section>
						<section id="adobe">
                            <div class="content_left">
                                <div class="flex-container">
                                    <div> <iframe width="500" height="315" src="https://www.youtube.com/embed/KmOS53w8d7U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div> 
                            </div>
							<div class="content">
								<div class="inner">
                                    <a href="https://youtu.be/KmOS53w8d7U"><h2>OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution (ICCV '21)</h2></a>
                                        
									<p>Our work to help fight against fake news was accepted at ICCV 2021!  We created an algorithm that creates a unique image fingerprint that can tell whether an image has been manipulated (Photoshopped), which we call OSCAR-Net.  Given any image in the wild, the algorithm creates a scene graph and uses graph neural networks and transformer encoders to learn an embedding of object visual features and their spatial relationships. We reached state-of-the-art of the PSBattles dataset.<br>
                                    </p>
                                    
                                    <ul class="actions">
                                        <li><a href="https://arxiv.org/abs/2108.03541" class="button">Arxiv Paper</a></li>
                                    </ul>

									<ul class="actions">
										<li><a href="https://exnx.github.io/oscar/" class="button">Project Website</a></li>
									</ul>
								</div>
							</div>
						</section>

						<section id="cpr">
                            <div class="content_left">
                                <div class="flex-container">
                                    <div> <iframe width="500" height="315" src="https://www.youtube.com/embed/1nMy7Gv1LYQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div> 
                            </div>
							<div class="content">
								<div class="inner">
                                    <a href="https://www.youtube.com/embed/1nMy7Gv1LYQ"><h2>Chest compression quality prediction from "in-the-wild" videos using self-supervised learning</h2></a>
                                        
									<p><br>
                                    Working with the Partnership in AI-assisted care in Fei-Fei Li's lab, we talked to clinicians to identify opportunities for AI to help patients in high-impact settings. Performing chest compression during CPR can vary widely in quality, leading to unneccary death. We trained a vision model to detect and rate the quality of chest compressions in order to provide real-time feedback. Our model can predict the rate of compression and detect when too much time has passed in between compressions, both key clinical metrics correlated to survival rates. We collected and annotated raw CPR videos from YouTube, and used self-supervision to increase the data signal, in particular for measuring the rate of compression.
                                    </p>
								</div>
							</div>
						</section>                                        
						<section>
                            <div class="content_left">
                                <img src="static/images/depth_video.gif" width="500" height="280" alt="depth_video" align="middle"/>
                            </div>
							<div class="content">
								<div class="inner">

                                    <a href="https://github.com/exnx/footprints"><h2>Dataset for egocentric walkable floor space detection</h2></a>
                                
									<p> I rotated with Leo Guibas' Geometric Computing Lab, and worked on the multi-camera perception system for an exoskeleton suit to help people walk and avoid obstacles. I created a dataset for egocentric walkable floor space detection using sparse footprint signals. Here's a sample of the depth video collected.
                                    </p>
                                    
									<ul class="actions">
										<li><a href="https://github.com/exnx/footprints" class="button">Github repo</a></li>
									</ul>
								</div>
							</div>
						</section>

						<section>
                            <div class="content_left">
				    			<a href="stool_classification_with_metric_learning.pdf" download=""><img src="static/images/stool_research.png" width="400" height="300" alt="Abstract" align="middle"/></a>
                            </div>
							<div class="content">
								<div class="inner">

                                    <a href="stool_classification_with_metric_learning.pdf"><h2>Stool Classification Using Deep Metric Learning</h2></a>
                                    
									<p>I researched human stool classification using deep metric learning at Cornell Tech.  You can read about our methods in the paper here.  My team crowdsourced a dataset and had three physicians annotate images of human stool.  We then tried several architectures and techniques in deep learning, and ultimately we were able to predict a key clinical metric, the Bristol Stool Scale, with near-doctor level accuracy.<br> 
                                    </p>
                                    
									<ul class="actions">
										<li><a href="stool_classification_with_metric_learning.pdf" class="button">Download Publication</a></li>
									</ul>
								</div>
							</div>
						</section>

						<section>
                            <div class="content_left">
			     				<a href="mcnair_journal_2014.pdf" download=""><img src="static/images/mcnair_research.png" width="400" height="300" alt="Abstract" align="middle"/></a>
                            </div>
							<div class="content">
								<div class="inner">
                                    <!-- <a href="https://github.com/exnx/drowsycam"><h2>Seismically-resistant Bridge Columns</h2></a> -->

		                            <ul class="actions">
										<li><a href="mcnair_journal_2014.pdf" ><h2>Seismically-resistant Bridge Columns</h2></a></li>
									</ul>

									<p>I worked on a research collaboration between UC Berkeley and the Tokyo Institute of Technology researching seismically-resistant bridge columns.  I built a finite-element model to predict the stress-strain relation and failure modes of a novel interlocking steel reinforcement column.  My work was published in the 2014 Berkeley McNair Scholar Journal.
                                    </p>
	                            <ul class="actions">
									<li><a href="mcnair_journal_2014.pdf" download="" class="button">Download Publication</a></li>
								</ul>
								</div>
							</div>
						</section>
					</section>
	
				<!-- two -->
			
					<section id="two" class="wrapper style2 spotlights">
						<section>
                            <div class="content_left">
                                <div class="flex-container">
                                    <div> <iframe width="500" height="315" src="https://www.youtube.com/embed/nq-0n8jkGRk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                                </div> 
                            </div>
                            
                            <div class="content">
								<div class="inner">
                                    <a href="http://auggi.ai"><h2>Auggi AI</h2></a>
                                                        
									<p>Auggi is a startup out of Cornell that's passionate about gut health.  I worked on the technology that allows you to take a picture of your stool on your phone and automatically characterize and extract clinical data from that image.  
									<br> 
									<br> 

<!--									Try it out, anonymously.-->
                                    
                                    <br> 
                                        
                                    </p>
<!--
									<ul class="actions">
										<li><a href="http://train.auggi.ai" class="button">Try Auggi Here</a></li>
									</ul>
-->
								</div>
							</div>        
						</section>

						<section>
                            <div class="content_left">
                              <img src="static/images/video_demo.gif" width="500" height="280" alt="Attention model demo" align="middle"/>
                            </div>
							<div class="content">
								<div class="inner">

                                    <a href="http://tvisioninsights.com"><h2>TVision</h2></a>
                                    
									<p>TVision is a startup in NYC disrupting the TV analytics industry. I worked on a prototype to analyze how people watched TV in their living rooms as part of opt-in home studies. I used face detection and head pose algorithms to predict a person's level of attention second by second while watching shows or ads.<br> 
                                    
                                    <br> 
                                    
                                    Here's one of the prototypes I built for TVision in the summer of 2018.
                                    
                                    </p>
									<ul class="actions">
										<li><a href="https://www.tvisioninsights.com/" class="button">TVision homepage</a></li>
									</ul>
								</div>
							</div>
						</section>

						<section>
                            <div class="content_left">
                                <div align="middle"> <iframe width="500" height="315" src="https://www.youtube.com/embed/-oCpN1JeWj8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
                            </div>
                            
							<div class="content">
								<div class="inner">
                                    <a href="https://github.com/exnx/air-cam"><h2>AirCam</h2></a>
									<p>With AirCam, you can ask "hey Google, where is the T.V. remote?" and it will verbally tell you where it is.  AirCam is a system that gives smart speakers "eyes" using an overhead camera.  On voice command, it will turn on a camera and find everyday objects you lose around your living room, including your phone and keys. <br> 
                                        
                                    <br>     
                                        
                                    AirCam was an applied research project with Professor Serge Belongie at Cornell and demoed at Cornell Tech's Open studio in December 2018.</p>
									<ul class="actions">
										<li><a href="https://github.com/exnx/air-cam" class="button">Github Code</a></li>
									</ul>
								</div>
							</div>
						</section>

					</section>
                
				<!-- three -->
					<section id="three" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Education</h2>
							<div class="features">
								<section>
                                    
									<h3>Stanford University, PhD Bioengineering + AI (2020-)</h3>

									<p>AI in vision, language and biology</p>

									<h3>Cornell University, MEng Computer Science (2019)</h3>

									<p>Computer vision and machine learning</p>
        
<!--                                    <h5>June 2018 to August 2018</h5>-->

									<h3>Stanford University, MS Civil Engineering (2009)</h3>
                                    
									<p>Sustainable construction and design</p>

<!--                                    <h5>June 2018 to August 2018</h5>-->
                                    
									<h3>UC Berkeley, BS Civil Engineering (2007)</h3>
                                    
									<p>Earthquare engineering</p>
            
                                    
								</section>
                            </div>
						</div>
					</section>
                
                
                
                
                
                
                

				<!-- four -->
					<section id="four" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Work Experience</h2>
							<div class="features">
								<section>

									<h3>Google Research (Jun 2022 - Sep 2022)</h3>
                                    <h4>Research Intern</h4>
									<p>Multimodal image inversion & generation.</p>                                     
									<h3>Google Research (Jun 2021 - Nov 2021)</h3>
                                    <h4>Research Intern</h4>
									<p>Multimodal content generation (simultaneous video + audio).</p>
                                    
									<h3>Adobe Research (Jun 2020 - Dec 2020)</h3>
                                    <h4>Deep Learning Research Intern</h4> 
									<p>Detecting authentic and tampered images for journalists using deep learning on the Content Authentication Initiative. Using scene graphs and graph neural networks to model discrepancies in objects and their spatial relationships in images.</p>

									<h3>Facebook AI (Jun 2019 - Apr 2020)</h3>
                                    <h4>Computer Vision Researcher</h4> 
<!--                                    <h5>June 2018 to August 2018</h5>-->
									<p>Image tampering and fake ID detection using deep learning and image forensics. Self-supervision, anomaly detection, metric learning, representation learning.</p>
<!--									<span class="icon major fa-code"></span>-->
									<h3>TVision</h3>
                                    <h4>Computer Vision Engineer Intern</h4> 
<!--                                    <h5>June 2018 to August 2018</h5>-->
									<p>A startup measuring at-home TV viewership and attention of shows and ads using computer vision in the living room</p>
                                    
									<h3>Power Advocate</h3>
                                    <h4>Energy Consultant Manager</h4> 
<!--                                    <h5>June 2018 to August 2018</h5>-->
									<p>Strategic sourcing consultant for energy companies</p>
                                    
									<h3>Aspen Environmental Group</h3>
                                    <h4>Energy Analyst</h4> 
<!--                                    <h5>June 2018 to August 2018</h5>-->
									<p>Energy policy analyst for regulatory agencies in California</p>
                                    
									<h3>Calera Corporation</h3>
                                    <h4>Lab Engineer</h4> 
<!--                                    <h5>June 2018 to August 2018</h5>-->
									<p>A start up company developing green cement products from power plant flue gas CO2 and seawater</p>                                    
 
									<h3>Curtins Consulting</h3>
                                    <h4>Structural Engineer Intern</h4> 
<!--                                    <h5>June 2018 to August 2018</h5>-->
									<p>Structural analysis of buildings in the London area</p>
           
									<h3>University of Tokyo</h3>
                                    <h4>Earthquake Engineer Research Intern</h4> 
<!--                                    <h5>June 2018 to August 2018</h5>-->
									<p>Researching earthquake-resistant bridge designs in Tokyo</p>
                                    
									<h3>Pacific Earthquake Engineering Research Center</h3>
                                    <h4>Structural Engineer Research Intern</h4> 
<!--                                    <h5>June 2018 to August 2018</h5>-->
									<p>Testing new earthquake-resistant concrete materials</p>
                                    
                                    
                                    
								</section>
                            </div>
						</div>
					</section>
                
                
                
<!--
				 Three 
					<section id="three" class="wrapper style1 fade-up">
						<div class="inner">
							<h2>Get in touch</h2>
							<p>Phasellus convallis elit id ullamcorper pulvinar. Duis aliquam turpis mauris, eu ultricies erat malesuada quis. Aliquam dapibus, lacus eget hendrerit bibendum, urna est aliquam sem, sit amet imperdiet est velit quis lorem.</p>
							<div class="split style1">
								<section>
									<form method="post" action="#">
										<div class="fields">
											<div class="field half">
												<label for="name">Name</label>
												<input type="text" name="name" id="name" />
											</div>
											<div class="field half">
												<label for="email">Email</label>
												<input type="text" name="email" id="email" />
											</div>
											<div class="field">
												<label for="message">Message</label>
												<textarea name="message" id="message" rows="5"></textarea>
											</div>
										</div>
										<ul class="actions">
											<li><a href="" class="button submit">Send Message</a></li>
										</ul>
									</form>
								</section>
								<section>
									<ul class="contact">
										<li>
											<h3>Address</h3>
											<span>12345 Somewhere Road #654<br />
											Nashville, TN 00000-0000<br />
											USA</span>
										</li>
										<li>
											<h3>Email</h3>
											<a href="#">user@untitled.tld</a>
										</li>
										<li>
											<h3>Phone</h3>
											<span>(000) 000-0000</span>
										</li>
										<li>
											<h3>Social</h3>
											<ul class="icons">
												<li><a href="#" class="fa-twitter"><span class="label">Twitter</span></a></li>
												<li><a href="#" class="fa-facebook"><span class="label">Facebook</span></a></li>
												<li><a href="#" class="fa-github"><span class="label">GitHub</span></a></li>
												<li><a href="#" class="fa-instagram"><span class="label">Instagram</span></a></li>
												<li><a href="#" class="fa-linkedin"><span class="label">LinkedIn</span></a></li>
											</ul>
										</li>
									</ul>
								</section>
							</div>
						</div>
					</section>

			</div>
-->


		 <!-- Footer  -->
			<footer id="footer" class="wrapper style1-alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; All rights reserved.</li>
					</ul>
				</div>
			</footer>


		<!-- Scripts -->
			<script src="static/js/jquery.min.js"></script>
			<script src="static/js/jquery.scrollex.min.js"></script>
			<script src="static/js/jquery.scrolly.min.js"></script>
			<script src="static/js/browser.min.js"></script>
			<script src="static/js/breakpoints.min.js"></script>
			<script src="static/js/util.js"></script>
			<script src="static/js/main.js"></script>

	</body>
</html>